# Title

Cached file reading, minimized lock aquisition and improved performance

# Summary

Within this commit, I provided proper caching for file reading, minimized lock acquisition, and some degree of randomized lock acquisition timing to improve the performance.

# Technical details

The first, and the most obvious issue is that both package and idea threads are reading the full file every time it tries to retrieve any entry, meaning a huge period of time is wasted during IO. In order to avoid this problem, each idea/package worker caches the file content upon initialization, since files are naturally small in most test cases, it is not neccessary to cache it in main (as it will not provide a high amount of performance boost)

Another problem is the frequency of lock acquisition. Based on the flame graph, it seems like the process spents most of its time spinning and waiting for lock acquisition, so it is better to acquire lock less time and commit more changes, although, it would not be ideal to acquire the lock for too long. Thus, some part of the code will hold lock for longer, but not extensively long to cause starvation. 

Similarly, some part of the code is also provided with some level of randomization to avoid lock collision. Task with high lock usage typically get a randmized key denoting how often do they commit their changes, with this approach, we saw some slight performance boost when it comes to spin lock, but not by much.

# Testing for correctness

In order to verify correctness, I kept a original copy of the repo with ensure the output hash is equivalent to the new hash generated by the new repo, which indeed helped me caught on to a bug related to array index.

# Testing for performance.

I tested against 2 type of argument set, the default "80 2 4000 6 6" and the ultra big "18000 30 180000 60 50". The default was a good enough matrix until I finished caching all the content, but soon I got a 20 + times performance improvement which made the initialization process more costy than the actual execution. Thus, we upgrade to the beefy, "18000 30 180000 60 50" setting.

Under the new setting, we can finally see some performance changes, while the original version runs for  84.878 s on average to execute the entire system, the new system takes only about 700ms! (or, 600ms on a good day, and about 800ms on a bad day.) This means we have an average performance boost of 120 times, which is quiet nice. According to the flame graph, we can see that minimum amount of time is spend on spinning and most of the time is spend on either decoding/encoding/receiving/transmitting. since most of these operation are locked by nature, there isn't much to improve there (or I should say we are informed not to touch the communication method, so I left it as it is)

It should be worth nothing that if you have too many threads, a large quantity of work is actually being wasted during thread communication, so the performance of both version become similar, or if you have too little work in general, the intiailization cost takes over and the performance become not similar, for exmaple, "1800 30 18000 60 50" isn't a great parameter set as too many workers are created for too simple of a task, making the new version only 4 times as fast as the original, while "1800 30 180000 60 50" serves as a much better example, similarly, "80 2 4000 6 6" is also pretty decent. (note that the final flame graph is created with parameter "1800 30 180000 60 50")